#version 420
#extension GL_ARB_texture_query_lod : enable

#include <GBuffer/GBuffer.glsl>

#include "GridHelpers.glsl"

//#define DDGI

uniform_Texture(sampler2D, gbuffer_WS_RAY_ORIGIN_);

uniform_GBuffer(gbuffer_);

uniform float energyPreservation;

uniform IrradianceField irradianceFieldSurface;

out Color3 E_lambertianIndirect;


struct FScreenProbeSample
{
	ivec2 AltasCoord[4];
	vec4 weights;
};

uniform int screenProbeDownsampleFactor;
uniform int   adaptiveProbeNum;
uniform float viewport_width;
uniform float viewport_height;

// Texture
uniform sampler2D   ws_positionTexture;
uniform sampler2D   depthTexture;
uniform sampler2D   ws_normalTexture;
uniform sampler2D   adaptiveProbeSSPosData;
uniform sampler2D   screenTileHeaderData;
uniform sampler2D   screenTileProbeIndex;

ivec2 GetAdaptiveProbeCoord(ivec2 screenTileCoord, int adaptiveProbeListIndex){

	ivec2 adaptiveProbeCoord = ivec2(adaptiveProbeListIndex % screenProbeDownsampleFactor, adaptiveProbeListIndex / screenProbeDownsampleFactor);
	//return ScreenTileCoord * ScreenProbeDownsampleFactor + AdaptiveProbeCoord;
	return ivec2(adaptiveProbeCoord.x * viewport_width / screenProbeDownsampleFactor, adaptiveProbeCoord.y * viewport_height / screenProbeDownsampleFactor) + screenTileCoord;

}

// 找到该点对应的4个uniform probe
void CalculateUniformUpsampleInterpolationWeights(ivec2 screenCoord, vec3 wsPosition, float sceneDepth, vec3 worldNormal, out ivec2 screenTileCoord00, out vec4 interpolationWeights){
    
    // clamp(screenCoord, 0.0f, ivec2(viewport_width - 1, viewport_height - 1));
	ivec2 screenProbeFullResScreenCoord = ivec2(clamp(screenCoord.x, 0.0f,viewport_width-1.0f), clamp(screenCoord.y, 0.0f, viewport_height-1));
    int tileSize = screenProbeDownsampleFactor;
     
    ivec2 screenProbeViewSize = ivec2((int) viewport_width / screenProbeDownsampleFactor, (int) viewport_height / screenProbeDownsampleFactor);
    // ScreenTileCoord00 = min(ScreenProbeFullResScreenCoord / ScreenProbeDownsampleFactor, (uint2) ScreenProbeViewSize - 2);
    screenTileCoord00.x = (screenProbeFullResScreenCoord / tileSize).x < (screenProbeViewSize.x - 2) ? (screenProbeFullResScreenCoord / tileSize).x : (screenProbeViewSize.x - 2);
    screenTileCoord00.y = (screenProbeFullResScreenCoord / tileSize).y < (screenProbeViewSize.y - 2) ? (screenProbeFullResScreenCoord / tileSize).y : (screenProbeViewSize.y - 2);


	int bilinearExpand = 1;
	vec2 bilinearWeights = (screenProbeFullResScreenCoord - screenTileCoord00 * tileSize + bilinearExpand) / (float)(tileSize + 2 * bilinearExpand);

	vec4 cornerDepths;

    
    cornerDepths.x = texelFetch(depthTexture, screenTileCoord00 * tileSize, 0).r;
	cornerDepths.y = texelFetch(depthTexture, ivec2(screenTileCoord00 + ivec2(1, 0)) * tileSize, 0).r;
    cornerDepths.z = texelFetch(depthTexture, ivec2(screenTileCoord00 + ivec2(0, 1)) * tileSize, 0).r;
    cornerDepths.w = texelFetch(depthTexture, ivec2(screenTileCoord00 + ivec2(1, 1)) * tileSize, 0).r;


	interpolationWeights = vec4(
		(1 - bilinearWeights.y) * (1 - bilinearWeights.x),
		(1 - bilinearWeights.y) * bilinearWeights.x,
		bilinearWeights.y * (1 - bilinearWeights.x),
		bilinearWeights.y * bilinearWeights.x);

	vec4 depthWeights;
    vec4 scenePlane = vec4(worldNormal, dot(wsPosition, worldNormal));

    vec3 position00 = texelFetch(ws_positionTexture, screenTileCoord00 * tileSize, 0).rgb;
    vec3 position10 = texelFetch(ws_positionTexture, (screenTileCoord00 + ivec2(1, 0)) * tileSize, 0).rgb;
    vec3 position01 = texelFetch(ws_positionTexture, (screenTileCoord00 + ivec2(0, 1)) * tileSize, 0).rgb;
    vec3 position11 = texelFetch(ws_positionTexture, (screenTileCoord00 + ivec2(1, 1)) * tileSize, 0).rgb;

    vec4 planeDistances;
    planeDistances.x = abs(dot(vec4(position00, -1), scenePlane)); // 法线上投影的距离
    planeDistances.y = abs(dot(vec4(position10, -1), scenePlane));
    planeDistances.z = abs(dot(vec4(position01, -1), scenePlane));
    planeDistances.w = abs(dot(vec4(position11, -1), scenePlane));
        
    vec4 relativeDepthDifference = planeDistances / sceneDepth;

    // cornerDepths 没有实际贡献，因为均匀采样各点距shading point距离相同
    depthWeights = ((cornerDepths.x > 0) && (cornerDepths.y > 0)  && (cornerDepths.z > 0) && (cornerDepths.w > 0)) ? exp2(-10000.0f * (relativeDepthDifference * relativeDepthDifference)) : vec4(0,0,0,0);

    interpolationWeights *= depthWeights;
}



void CalculateUpsampleInterpolationWeights(ivec2 screenCoord, vec3 wsPosition, float sceneDepth, vec3 worldNormal, out FScreenProbeSample screenProbeSample){

    ivec2 screenTileCoord00;
    CalculateUniformUpsampleInterpolationWeights(screenCoord, wsPosition, sceneDepth, worldNormal, screenTileCoord00, screenProbeSample.weights);
    int tileSize = screenProbeDownsampleFactor; 
    screenProbeSample.AltasCoord[0] = screenTileCoord00;
	screenProbeSample.AltasCoord[1] = screenTileCoord00 + ivec2(1, 0);
	screenProbeSample.AltasCoord[2] = screenTileCoord00 + ivec2(0, 1);
	screenProbeSample.AltasCoord[3] = screenTileCoord00 + ivec2(1, 1);

    float epsilon = .01f;
    vec4 scenePlane = vec4(worldNormal, dot(wsPosition, worldNormal)); 

    // 上面拿到了均匀的probe的weight下面看看用adaptive的probe会不会更好
    // 替换掉4个均匀probe的其中一个
    for (int cornerIndex = 0; cornerIndex < 4; cornerIndex++)
    {
        // 如果四个中的其中一个权重太小，则由adaptive替换
        if (screenProbeSample.weights[cornerIndex] <= epsilon)
        {
            ivec2 screenTileCoord = screenTileCoord00 + ivec2(cornerIndex % 2, cornerIndex / 2);
            
            // 这个被替换的tile中有多少个adaptive probe
            //int numAdaptiveProbes = screenTileHeaderData[screenTileCoord.y * ((int) viewport_width / tileSize) + screenTileCoord.x];
            int numAdaptiveProbes = (int)texelFetch(screenTileHeaderData, screenTileCoord, 0).r;

            // 之前已经计算过均匀的probe带来的weight，下面试试取Adaptive probe插值
            for (int adaptiveProbeListIndex = 0; adaptiveProbeListIndex < numAdaptiveProbes; adaptiveProbeListIndex++)
            {
                ivec2 adaptiveProbeCoord = GetAdaptiveProbeCoord(screenTileCoord, adaptiveProbeListIndex);

                //int adaptiveProbeIndex = screenTileProbeIndex[(int)(adaptiveProbeCoord.y * viewport_width + adaptiveProbeCoord.x)];
                int adaptiveProbeIndex = (int)texelFetch(screenTileProbeIndex, adaptiveProbeCoord, 0).r;

                //ivec2 screenProbeScreenPosition =  (ivec2) adaptiveProbeSSPosData[adaptiveProbeIndex].rg;
                int uniformProbeCountX  = ((int) viewport_width) / tileSize;
                int uniformProbeCountY  = ((int) viewport_height) / tileSize;
                int uniformProbeCount = uniformProbeCountX * uniformProbeCountY;
                ivec2 screenProbeAltasCoord = ivec2((int)((adaptiveProbeCoord + uniformProbeCount)/ uniformProbeCountX), 
                                                    (adaptiveProbeCoord + uniformProbeCount)% uniformProbeCountX);
                ivec2 screenProbeScreenPosition = (ivec2)texelFetch(adaptiveProbeSSPosData,screenProbeAltasCoord - ivec2(0,uniformProbeCountY),0).rg;
                float probeDepth = texelFetch(depthTexture, screenProbeScreenPosition, 0).r;
                
                float newDepthWeight = 0;
                bool bPlaneWeighting = true;
                if (bPlaneWeighting)
                {
                    vec3 probePosition = texelFetch(ws_positionTexture, screenProbeScreenPosition, 0).rgb;
                    float planeDistance = abs(dot(vec4(probePosition, -1), scenePlane)); // 投影到平面法向量上的长度 scenePlane.w = wsposition . wsnormal
                    float relativeDepthDifference = planeDistance / sceneDepth;
                    newDepthWeight = exp2(-10000.0f * (relativeDepthDifference * relativeDepthDifference));
                }
                else
                {
                    float depthDifference = abs(probeDepth - sceneDepth);
                    float relativeDepthDifference = depthDifference / sceneDepth;
                    newDepthWeight = probeDepth > 0 ? exp2(-100.0f * (relativeDepthDifference * relativeDepthDifference)) : 0;
                }

                vec2 distanceToScreenProbe = abs(screenProbeScreenPosition - screenCoord); // 屏幕空间上的距离
                float newCornerWeight = 1.0f - clamp(min(distanceToScreenProbe.x, distanceToScreenProbe.y) / (float) screenProbeDownsampleFactor, 0,1);
                float newInterpolationWeight = newDepthWeight * newCornerWeight;

                if (newInterpolationWeight > screenProbeSample.weights[cornerIndex])
                {
                    screenProbeSample.weights[cornerIndex] = newInterpolationWeight;
                    screenProbeSample.AltasCoord[cornerIndex] = screenProbeAltasCoord;
                }
            }
        }
    }
}

vec2 textureCoordFromDirection(vec3 dir, int probeIndex, int fullTextureWidth, int fullTextureHeight, int probeSideLength)
{
    vec2 normalizedOctCoord = octEncode(normalize(dir));
    vec2 normalizedOctCoordZeroOne = (normalizedOctCoord + vec2(1.0f)) * 0.5f;

    // Length of a probe side, plus one pixel on each edge for the border
    float probeWithBorderSide = (float)probeSideLength + 2.0f;

    vec2 octCoordNormalizedToTextureDimensions = (normalizedOctCoordZeroOne * (float)probeSideLength) / vec2((float)fullTextureWidth, (float)fullTextureHeight);

    int probesPerRow = (fullTextureWidth - 2) / (int)probeWithBorderSide;

    // Add (2,2) back to texCoord within larger texture. Compensates for 1 pix 
    // border around texture and further 1 pix border around top left probe.
    vec2 probeTopLeftPosition = vec2(mod(probeIndex, probesPerRow) * probeWithBorderSide,
        (probeIndex / probesPerRow) * probeWithBorderSide) + vec2(2.0f, 2.0f);

    vec2 normalizedProbeTopLeftPosition = vec2(probeTopLeftPosition) / vec2((float)fullTextureWidth, (float)fullTextureHeight);

    return vec2(normalizedProbeTopLeftPosition + octCoordNormalizedToTextureDimensions);
}

Irradiance3 GetScreenProbeIrradiance(ivec2 screenProbeAltasCoord, float2 IrradianceProbeUV)
{
	//float2 IrradianceProbeUVCoord = IrradianceProbeUV * IRRADIANCE_PROBE_RES + 1.0f;
    float probeSideWithBorder = 10.0f;
    float probeSide = 8.0f;
    float2 screenProbeAtlasBufferSize = float2(viewport_width/screenProbeDownsampleFactor,viewport_height/screenProbeDownsampleFactor);
    float2 IrradianceProbeUVCoord = IrradianceProbeUV * probeSide + 1.0f;
	float2 AtlasUV = (screenProbeAltasCoord * probeSideWithBorder + IrradianceProbeUVCoord) / (screenProbeAtlasBufferSize * probeSideWithBorder);
	//return ScreenProbeIrradianceWithBorder.SampleLevel(GlobalBilinearClampedSampler, AtlasUV, 0).xyz;
    return texture(irradianceFieldSurface.irradianceProbeGridbuffer, AtlasUV).rgb;
}

void main()
{

#ifndef DDGI
    ivec2 screenCoord = ivec2(gl_FragCoord.xy);
    vec3 wsPos = texelFetch(ws_positionTexture, screenCoord, 0).rgb;
    float depth = texelFetch(depthTexture, screenCoord, 0).r;
    vec3 wsNormal = texelFetch(ws_normalTexture, screenCoord, 0).rgb;

    FScreenProbeSample screenProbeSample;
    CalculateUpsampleInterpolationWeights(screenCoord, wsPos, depth, wsNormal, screenProbeSample);
    float epsilon = .01f;
    screenProbeSample.weights /= max(dot(screenProbeSample.weights, vec4(1,1,1,1)), epsilon);

    vec2 normalizedOctCoord = octEncode(normalize(wsNormal));
    vec2 irradianceProbeUV = (normalizedOctCoord + vec2(1.0f)) * 0.5f;

    Irradiance3 irradiance = GetScreenProbeIrradiance(screenProbeSample.AltasCoord[0], irradianceProbeUV) * screenProbeSample.weights.x;
    irradiance += GetScreenProbeIrradiance(screenProbeSample.AltasCoord[1], irradianceProbeUV) * screenProbeSample.weights.y;
	irradiance += GetScreenProbeIrradiance(screenProbeSample.AltasCoord[2], irradianceProbeUV) * screenProbeSample.weights.z;
	irradiance += GetScreenProbeIrradiance(screenProbeSample.AltasCoord[3], irradianceProbeUV) * screenProbeSample.weights.w;

    //E_lambertianIndirect = irradiance;
    E_lambertianIndirect = screenProbeSample.weights.xyz;

#else
    ivec2 C = ivec2(gl_FragCoord.xy);
    Vector3 wsN = texelFetch(gbuffer_WS_NORMAL_buffer, C, 0).xyz;

    if (dot(wsN, wsN) < 0.01)
	{
        E_lambertianIndirect = Color3(0);
        return;
    }
    
    Point3 wsPosition = texelFetch(gbuffer_WS_POSITION_buffer, C, 0).xyz;

    // View vector
#ifdef RT_GBUFFER
    Vector3 w_o = normalize(texelFetch(gbuffer_WS_RAY_ORIGIN_buffer, C, 0).xyz - wsPosition);
#else
	Vector3 w_o = normalize(gbuffer_camera_frame[3] - wsPosition);
#endif


    ivec3 baseGridCoord = baseGridCoord(irradianceFieldSurface, wsPosition);
    Point3 baseProbePos = gridCoordToPosition(irradianceFieldSurface, baseGridCoord);
    Irradiance3 sumIrradiance = Irradiance3(0);
    float sumWeight = 0.0;

    // alpha is how far from the floor(currentVertex) position. on [0, 1] for each axis.
    Vector3 alpha = clamp((wsPosition - baseProbePos) / irradianceFieldSurface.probeStep, Vector3(0), Vector3(1));

    // Iterate over adjacent probe cage
    for (int i = 0; i < 8; ++i)
	{
        // Compute the offset grid coord and clamp to the probe grid boundary
        // Offset = 0 or 1 along each axis
        GridCoord  offset = ivec3(i, i >> 1, i >> 2) & ivec3(1);
        GridCoord  probeGridCoord = clamp(baseGridCoord + offset, GridCoord(0), GridCoord(irradianceFieldSurface.probeCounts - 1));
        ProbeIndex p = gridCoordToProbeIndex(irradianceFieldSurface, probeGridCoord);

        // Make cosine falloff in tangent plane with respect to the angle from the surface to the probe so that we never
        // test a probe that is *behind* the surface.
        // It doesn't have to be cosine, but that is efficient to compute and we must clip to the tangent plane.
        Point3 probePos = gridCoordToPosition(irradianceFieldSurface, probeGridCoord);

		Vector3 probeToPoint = wsPosition - probePos + (wsN + 3.0 * w_o) * irradianceFieldSurface.normalBias;
        Vector3 dir = normalize(-probeToPoint);

        // Compute the trilinear weights based on the grid cell vertex to smoothly
        // transition between probes. Avoid ever going entirely to zero because that
        // will cause problems at the border probes. This isn't really a lerp. 
        // We're using 1-a when offset = 0 and a when offset = 1.
        Vector3 trilinear = lerp(1.0 - alpha, alpha, offset);
        float weight = 1.0;


        // Smooth backface test
        {
            // Computed without the biasing applied to the "dir" variable. 
            // This test can cause reflection-map looking errors in the image
            // (stuff looks shiny) if the transition is poor.
            Vector3 trueDirectionToProbe = normalize(probePos - wsPosition);
            weight *= square(max(0.0001, (dot(trueDirectionToProbe, wsN) + 1.0) * 0.5)) + 0.2;
        }
        
        // Moment visibility test
        {
            vec2 texCoord = textureCoordFromDirection(-dir,
                p,
                irradianceFieldSurface.depthTextureWidth,
                irradianceFieldSurface.depthTextureHeight,
                irradianceFieldSurface.depthProbeSideLength);

            float distToProbe = length(probeToPoint);

            float2 temp = texture(irradianceFieldSurface.meanMeanSquaredProbeGridbuffer, texCoord, 0).rg;
            float mean = temp.x;
            float variance = abs(square(temp.x) - temp.y);

            // http://www.punkuser.net/vsm/vsm_paper.pdf; equation 5
            // Need the max in the denominator because biasing can cause a negative displacement
            float chebyshevWeight = variance / (variance + square(max(distToProbe - mean, 0.0)));
                
            // Increase contrast in the weight 
            chebyshevWeight = max(pow3(chebyshevWeight), 0.0);

            weight *= (distToProbe <= mean) ? 1.0 : chebyshevWeight;
        }

        // Avoid zero weight
        weight = max(0.000001, weight);
                 
        Vector3 irradianceDir = wsN;

        vec2 texCoord = textureCoordFromDirection(normalize(irradianceDir),
            p,
            irradianceFieldSurface.irradianceTextureWidth,
            irradianceFieldSurface.irradianceTextureHeight,
            irradianceFieldSurface.irradianceProbeSideLength);

        Irradiance3 probeIrradiance = texture(irradianceFieldSurface.irradianceProbeGridbuffer, texCoord).rgb;

        // A tiny bit of light is really visible due to log perception, so
        // crush tiny weights but keep the curve continuous. This must be done
        // before the trilinear weights, because those should be preserved.
        const float crushThreshold = 0.2;
        if (weight < crushThreshold) {
            weight *= weight * weight * (1.0 / square(crushThreshold)); 
        }

        // Trilinear weights
        weight *= trilinear.x * trilinear.y * trilinear.z;

        // Weight in a more-perceptual brightness space instead of radiance space.
        // This softens the transitions between probes with respect to translation.
        // It makes little difference most of the time, but when there are radical transitions
        // between probes this helps soften the ramp.
#       if LINEAR_BLENDING == 0
            probeIrradiance = sqrt(probeIrradiance);
#       endif
        
        sumIrradiance += weight * probeIrradiance;
        sumWeight += weight;
    }

    Irradiance3 netIrradiance = sumIrradiance / sumWeight;

    // Go back to linear irradiance
#   if LINEAR_BLENDING == 0
        netIrradiance = square(netIrradiance);
#   endif
    netIrradiance *= energyPreservation;

    E_lambertianIndirect = 2 * pi * netIrradiance;
#endif
}
